{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T01:31:14.165649Z",
     "iopub.status.busy": "2025-04-06T01:31:14.165260Z",
     "iopub.status.idle": "2025-04-06T01:32:11.542752Z",
     "shell.execute_reply": "2025-04-06T01:32:11.542049Z",
     "shell.execute_reply.started": "2025-04-06T01:31:14.165619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "labels = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprise\",\n",
    "    \n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)  # RGB ì •ê·œí™”\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('/kaggle/input/rafdataset/RAF dataset/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder('/kaggle/input/rafdataset/RAF dataset/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T01:32:30.603694Z",
     "iopub.status.busy": "2025-04-06T01:32:30.603284Z",
     "iopub.status.idle": "2025-04-06T01:32:33.863182Z",
     "shell.execute_reply": "2025-04-06T01:32:33.862278Z",
     "shell.execute_reply.started": "2025-04-06T01:32:30.603670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–¼êµ´ ì €ì¥ ì™„ë£Œ: 2424x224.jpg\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image, ImageEnhance, ImageStat\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”\n",
    "mtcnn = MTCNN(keep_all=False, device='cpu')  # ë‹¨ì¼ ì–¼êµ´ë§Œ\n",
    "\n",
    "def auto_brightness(image, target_mean=250):\n",
    "    \"\"\"\n",
    "    í˜„ì¬ ì´ë¯¸ì§€ì˜ ë°ê¸° í‰ê· ì„ ì¸¡ì •í•´ì„œ,\n",
    "    target_mean(ì˜ˆ: 130)ì— ë§ê²Œ ë°ê¸° ë¹„ìœ¨ì„ ì¡°ì •í•´ì£¼ëŠ” í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    stat = ImageStat.Stat(image)\n",
    "    mean = stat.mean[0]  # í‘ë°± ì´ë¯¸ì§€ì¼ ë•ŒëŠ” ì±„ë„ì´ 1ê°œ\n",
    "\n",
    "    # ë°ê¸° ë³´ì • ë¹„ìœ¨ ê³„ì‚°\n",
    "    brightness_factor = target_mean / (mean + 1e-5)\n",
    "\n",
    "    # ë„ˆë¬´ ê³¼í•œ ë³´ì •ì€ ë°©ì§€ (ì•ˆì •í™” ë²”ìœ„ ì§€ì •)\n",
    "    brightness_factor = max(0.7, brightness_factor)\n",
    "\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    return enhancer.enhance(brightness_factor)\n",
    "\n",
    "\n",
    "image_path = 'Example.jpg' # í™•ì¸í•  ì´ë¯¸ì§€\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "image = auto_brightness(image, target_mean=250)\n",
    "\n",
    "\n",
    "# ì–¼êµ´ crop\n",
    "face = mtcnn(image)  # ê²°ê³¼: torch.Tensor [3, H, W]\n",
    "\n",
    "if face is not None:\n",
    "    # â¬‡ï¸ ì „ì²˜ë¦¬: Grayscale + Resize(48x48)\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224))\n",
    "    ])\n",
    "\n",
    "    # ì´ë¯¸ì§€ ê°’ ìŠ¤ì¼€ì¼ë§\n",
    "    face = (face * 255).clamp(0, 255).byte()\n",
    "\n",
    "    # Tensor â†’ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ì „ì²˜ë¦¬\n",
    "    face_pil = transforms.ToPILImage()(face)\n",
    "    face_gray_resized = transform(face_pil)\n",
    "\n",
    "    # ì €ì¥\n",
    "    face_gray_resized.save(\"224x224.jpg\")\n",
    "    print(\"ì–¼êµ´ ì €ì¥ ì™„ë£Œ: 2424x224.jpg\")\n",
    "\n",
    "else:\n",
    "    print(\"ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ê°ì • ì˜ˆì¸¡ ê²°ê³¼: Neutral (99.43%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from facenet_pytorch import MTCNN\n",
    "from model import EmotionSwin  # Swin ëª¨ë¸ ì •ì˜\n",
    "\n",
    "# ê°ì • ë¼ë²¨ ì •ì˜\n",
    "emotion_labels = {\n",
    "    0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\",\n",
    "    4: \"Neutral\", 5: \"Sad\", 6: \"Surprise\",\n",
    "}\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "model = EmotionSwin(num_classes=7).to(device)\n",
    "model.load_state_dict(torch.load('emotion_swin_last.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ì´ë¯¸ì§€ ê²½ë¡œ (48x48 grayscale ì´ë¯¸ì§€)\n",
    "image_path = '224x224.jpg'\n",
    "\n",
    "# ğŸ”„ ì „ì²˜ë¦¬: 1ì±„ë„ â†’ 3ì±„ë„ ë³µì œ â†’ Resize â†’ Tensor â†’ Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),         # í‘ë°± â†’ RGB ì±„ë„ ë³µì œ\n",
    "    transforms.Resize((224, 224)),                       # Swin ì…ë ¥ í¬ê¸° ë§ì¶¤\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)      # [-1, 1] ë²”ìœ„ ì •ê·œí™”\n",
    "])\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° & ì „ì²˜ë¦¬\n",
    "image = Image.open(image_path).convert('L')\n",
    "image = transform(image).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    predicted = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][predicted].item()\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ§  ê°ì • ì˜ˆì¸¡ ê²°ê³¼: {emotion_labels[predicted]} ({confidence * 100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7053023,
     "sourceId": 11281196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
