import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

labels = {
    0: "Angry",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Neutral",
    5: "Sad",
    6: "Surprise",
    
}

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)  # RGB ì •ê·œí™”
])

train_dataset = datasets.ImageFolder('RAF dataset/train', transform=transform)
test_dataset = datasets.ImageFolder('RAF dataset/test', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

# train_swin.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts  
import matplotlib.pyplot as plt
from tqdm import tqdm

# ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°
num_epochs = 30
batch_size = 32
learning_rate = 3e-4
num_classes = 7

# ğŸ’» ì¥ì¹˜ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ§  ëª¨ë¸ ì •ì˜
model = EmotionSwin(num_classes=num_classes).to(device)

# ğŸ¯ ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)

# â³ CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

# ğŸ“ˆ ì‹œê°í™”ë¥¼ ìœ„í•œ ê°’ ì €ì¥
loss_history = []
acc_history = []
lr_history = []

# ğŸ” í•™ìŠµ ë£¨í”„
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}]"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)

    # ğŸ¯ Epoch í‰ê·  ì €ì¥
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    loss_history.append(epoch_loss)
    acc_history.append(epoch_acc)
    lr_history.append(scheduler.get_last_lr()[0])

    print(f"âœ… Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}% | LR: {lr_history[-1]:.6f}")

    # ğŸ”„ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler.step()

# ğŸ’¾ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'emotion_swin.pth')
print("ğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: emotion_swin.pth")

# ğŸ“Š í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Loss', marker='o')
plt.plot(acc_history, label='Accuracy', marker='x')
plt.title('Training Loss & Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(lr_history, label='Learning Rate', color='orange')
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('LR')
plt.grid(True)

plt.tight_layout()
plt.savefig("swin_train_curve.png")
plt.show()