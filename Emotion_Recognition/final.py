import torch
from torchvision import transforms
from PIL import Image
import torch.nn.functional as F
from facenet_pytorch import MTCNN
from model import EmotionSwin  # Swin ëª¨ë¸ ì •ì˜

# ê°ì • ë¼ë²¨ ì •ì˜
emotion_labels = {
    0: "Angry", 1: "Disgust", 2: "Fear", 3: "Happy",
    4: "Neutral", 5: "Sad", 6: "Surprise",
}

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë¡œë”©
model = EmotionSwin(num_classes=7).to(device)
model.load_state_dict(torch.load('emotion_swin_last.pth', map_location=device))
model.eval()

# ì´ë¯¸ì§€ ê²½ë¡œ (48x48 grayscale ì´ë¯¸ì§€)
image_path = 'face_gray_48x48.jpg'

# ğŸ”„ ì „ì²˜ë¦¬: 1ì±„ë„ â†’ 3ì±„ë„ ë³µì œ â†’ Resize â†’ Tensor â†’ Normalize
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),         # í‘ë°± â†’ RGB ì±„ë„ ë³µì œ
    transforms.Resize((224, 224)),                       # Swin ì…ë ¥ í¬ê¸° ë§ì¶¤
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)      # [-1, 1] ë²”ìœ„ ì •ê·œí™”
])

# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° & ì „ì²˜ë¦¬
image = Image.open(image_path).convert('L')
image = transform(image).unsqueeze(0).to(device)  # [1, 3, 224, 224]

# ì˜ˆì¸¡
with torch.no_grad():
    outputs = model(image)
    probs = F.softmax(outputs, dim=1)
    predicted = torch.argmax(probs, dim=1).item()
    confidence = probs[0][predicted].item()

# ì¶œë ¥
print(f"ğŸ§  ê°ì • ì˜ˆì¸¡ ê²°ê³¼: {emotion_labels[predicted]} ({confidence * 100:.2f}%)")